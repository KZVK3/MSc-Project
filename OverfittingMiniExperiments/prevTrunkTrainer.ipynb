{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TrunkTrainer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bicCHm1Bjzcd","executionInfo":{"status":"ok","timestamp":1631537023350,"user_tz":-60,"elapsed":75151,"user":{"displayName":"Louis Robinson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07105536916175268980"}},"outputId":"d2c6ff97-7bbb-4a0b-8d0a-7205b1d53798"},"source":["from google.colab import drive\n","drive.mount('/content/drive',  force_remount=True)\n","import os, sys\n","folder_name = 'Generation2' #@param {type:\"string\"}\n","os.chdir(os.getcwd()+'/drive/MyDrive/%s'%folder_name)\n","\n","!pip -q install biopython ml-collections pytorch-lightning\n","\n","from data.data_transforms import transform, prepare_features\n","from data.loader import RNA\n","# from data.build_dataset import open_data, RNAData\n","from config import model_config, TRACK_CODES\n","from data.generate_fake_example import generate_fake_example\n","from MainTrunk import BaseTrunk\n","\n","import os, json, gzip, glob\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from pytorch_lightning import Trainer\n","from pytorch_lightning import seed_everything\n","from pytorch_lightning import Callback\n","from pytorch_lightning.utilities import rank_zero_info\n","\n","from psutil import virtual_memory\n","\n","seed_everything(42, workers=True)\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","f = gzip.open('data/dataset.gz','rb')\n","loaded_rnas = json.loads(gzip.decompress(f.read()).decode(\"utf-8\"))\n","all_data = {}\n","for dname, dat in loaded_rnas.items():\n","  all_data[dname] = {}\n","  for code,dictt in tqdm(dat.items()):\n","    all_data[dname][code] = RNA(**dictt)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[K     |████████████████████████████████| 2.3 MB 5.4 MB/s \n","\u001b[K     |████████████████████████████████| 88 kB 6.7 MB/s \n","\u001b[K     |████████████████████████████████| 922 kB 47.2 MB/s \n","\u001b[K     |████████████████████████████████| 636 kB 38.2 MB/s \n","\u001b[K     |████████████████████████████████| 119 kB 46.6 MB/s \n","\u001b[K     |████████████████████████████████| 282 kB 42.8 MB/s \n","\u001b[K     |████████████████████████████████| 829 kB 51.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 45.9 MB/s \n","\u001b[K     |████████████████████████████████| 142 kB 46.5 MB/s \n","\u001b[K     |████████████████████████████████| 294 kB 49.2 MB/s \n","\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["Global seed set to 42\n"]},{"output_type":"stream","name":"stdout","text":["Mon Sep 13 12:43:04 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 845/845 [00:00<00:00, 180798.19it/s]\n","100%|██████████| 707/707 [00:00<00:00, 92330.32it/s]\n","100%|██████████| 2273/2273 [00:00<00:00, 125024.96it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"H5k6aYMNv0X4"},"source":["```\n","alphafold:\n","  alphafolditeration:\n","    embeddings_and_evoformer:\n","      relpos\n","      seq_emb\n","      msa_emb\n","      (prev_emb)\n","      evoformer_iteration (48): {\n","        msa_row_attn_p_b\n","        col_glob_attnt\n","        msa_transition\n","        outer_prod_mean\n","        tri_mult_out\n","        tri_mult_in\n","        tri_attn_str\n","        tri_attn_end\n","        pair_transition\n","      }\n","    distogram_head\n","    (msa_head)\n","```\n","\n","# Training the Evoformer & Embeddings\n","\n","Let 1 epoch = 10K samples.\n","- Standard evoformer (8 layers)\n","- New distogram head\n","- Plot distograms as training progresses\n","- Progressively increase length of crops from 40 going up in 10's every 3 epochs \n","- Save weights and record validation set loss.\n","\n","# Training the Structure Module\n","\n","- Add in prev_emb and recycling\n","- Add in structure module"]},{"cell_type":"code","metadata":{"id":"GRbmFNCsOEwk"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from data.loader import RNA_const\n","\n","def filter_coord_data(all_data, min_len=7, min_mask_prop=0.4):\n","  ''' \n","  Check coords are valid:\n","  - Some distances must be above the first break size\n","  - Some distances must be below the last break size\n","  - Unmasked coords must be above min size\n","\n","  '''\n","  names = ['train','validation','hold-out']\n","\n","  def compute_average_origin_bond_length(data):\n","    lengths = {}\n","    for c, d in data.items():\n","      xm = np.array(data[c].atom_mask)[:,RNA_const.basis[0]]\n","      X = np.array(data[c].atom_positions)[xm==1,RNA_const.basis[0]]\n","      lns = ((X[1:]-X[:-1])**2).sum(-1)**0.5\n","      contig = len(max(bytes(list(xm.astype(int))).split(b'\\0')))\n","      lengths[c] = (lns, len(X), len(xm), contig)\n","    return lengths\n","  \n","  a,b,c = [compute_average_origin_bond_length(all_data[n]) for n in names]\n","  all_ = {**a,**b,**c}\n","  \n","  # filter out the infinity ones\n","  all_ = {k:(lns, l, m, ct) for k,(lns, l, m, ct) in all_.items() if np.mean(lns) < float('inf')}\n","\n","  # filter the too short ones\n","  all_ = {k:(lns, l, m, ct) for k,(lns, l, m, ct) in all_.items() if m > min_len}\n","\n","  # filter ones which have too little valid points\n","  all_ = {k:(lns, l, m, ct) for k,(lns, l, m, ct) in all_.items() if m > l*min_mask_prop}\n","\n","  av = [(a, np.mean(b), np.std(b)) for a,(b,*_) in all_.items()]\n","  codes, average_lens, stdevs = list(zip(*av))\n","\n","  print((np.mean(average_lens), np.std(average_lens), np.mean(stdevs), np.std(stdevs)))\n","  plt.hist(average_lens, bins=100)\n","  plt.show()\n","  plt.hist(stdevs, bins=100)\n","  plt.show()\n","\n","  return set(all_.keys())\n","  \n","good_examples = filter_coord_data(all_data, min_len=24)\n","\n","print(len(good_examples))\n","print([len(all_data[n]) for n in ['train','validation','hold-out']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7O_PNRDTTORK"},"source":["def plot_data(w,h):\n","  fig = plt.figure(figsize=(w*10, h*10))\n","  for k,(i,j) in enumerate([(a,b) for a in range(w) for b in range(h)]):\n","    if k<len(codes):\n","      code = codes[k]\n","      X = np.array(all_data['train'][code].atom_positions)[:,RNA_const.basis[0]]\n","      X = X[np.array(all_data['train'][code].atom_mask)[:,RNA_const.basis[0]]==1]\n","      ax = fig.add_subplot(h, w, k+1, projection='3d')\n","      ax.plot(*X.T, 'o-', lw=0.6)\n","      ax.axis('off')\n","      ax.set_title(code, fontsize=30)\n","  plt.tight_layout()\n","  plt.show()\n","\n","def conditions(c):\n","  xm = np.array(all_data['train'][c].atom_mask)[:,RNA_const.basis[0]]\n","  return xm.sum() >= 8\n","\n","def disply_plts():\n","  codes = list(all_data['train'].keys())\n","  codes = list(filter(conditions, codes))\n","  w, h = 14, 40\n","  plot_data(w,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUqWIbDQRB2m"},"source":["\n","class RNAData(Dataset):\n","  # res = ['1.5A','2.0A','2.5A','3.0A','3.5A','4.0A','20.0A','above 20']\n","  def __init__(self, \n","      data, config, prepare_features, transform, device='cpu', filter_msa=None, length=None, deterministic=False\n","    ):\n","    min_len = 8\n","    kw = {'rna':True, 'basis':(\"C4'\",'P','P'), 'basis_shifts':(0,1,0)}\n","    data = {k:d for k,d in data.items() if k in good_examples}\n","\n","    # go through data and filter MSA if e-val is to high\n","    if filter_msa is not None:\n","      msa_evals = json.loads(open('data/msa_evals.json','r').read())\n","      get_rid = {k for k, v in msa_evals.items() if v>filter_msa and k in data}\n","      print('filtered %d msa to %d msa with e-values better than %s'%(\n","          len(msa_evals), len(msa_evals)-len(get_rid), str(filter_msa)))\n","      for k in get_rid:\n","        d = data[k].__dict__\n","        d['msa'] = [d['sequence']]\n","        data[k] = RNA(**d)\n","      \n","    self.data = {k:prepare_features(v, device, **kw) for k,v in tqdm(data.items()) if v.num_res > min_len}\n","    # self.data = {k:{f:t[0] for f,t in feat.items()} for k, feat in self.data}\n","    self.ix2key = [k for k in self.data]\n","    self.config = config.data\n","    self.transform = transform\n","    self.ln = length if length is not None else len(self.ix2key)\n","\n","    # length_bins = [min_len,40,60,80,100]\n","    # l2b = {l:j-1 for j in range(1, len(length_bins)) for l in range(length_bins[j-1], length_bins[j])}\n","    # len_bins = [[] for _ in range(len(length_bins))]\n","    # for cde, btch in self.data.items()\n","    #   l = len(btch['seq_length'])\n","    #   len_bins[l2b[l] if l in l2b else -1].append(cde)\n","    # print('bin sizes: '+str([len(a) for a in len_bins]))\n","\n","    self.get = lambda i: torch.randint(0,len(self.ix2key), (1,)).item()\n","    if deterministic: self.get = lambda i:i\n","\n","  def __len__(self):\n","    return self.ln\n","  \n","  def __getitem__(self, idx):\n","    code = self.ix2key[self.get(idx)]\n","    batch = self.transform(*self.data[code], self.config, 1, 0, rna=True)\n","    batch = {f:t[0] for f,t in batch.items()}# only one forward pass\n","    batch['code'] = code\n","    return batch\n","    \n","def collate_fn(data):\n","  # it's already batched... so only use the first item (only item)\n","  return data[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCUwu0Zof-fj"},"source":["def instantiateAF(config, device):\n","  random_batch = generate_fake_example(0)\n","  random_batch = {f:t[0] for f,t in random_batch.items()}\n","\n","  af = BaseTrunk(config)\n","\n","  out, loss, info = af({k:v.to(device) for k,v in random_batch.items()})\n","  del loss, out\n","  return af\n","\n","def standard_init(config, device, TRACK_CODES, num_epochs, n_examples, checkpoint_version=None):\n","  af = instantiateAF(config, device)\n","  af.eval()\n","  af.is_training = True\n","  af.compute_loss = True\n","\n","  af.set_optim_config(opt_cfg, num_epochs, n_examples)\n","  af.track_codes(TRACK_CODES)\n","  af = af.to(device)\n","\n","  if checkpoint_version is not None:\n","    print('loading from checkpoint: '+str(checkpoint_version))\n","    path = 'lightning_logs/version_%s/checkpoints'%checkpoint_version\n","    [path] = list(glob.glob(path+'/*.ckpt'))\n","    af.load_state_dict(torch.load(path)[\"state_dict\"])\n","    print('done')\n","\n","  return af\n","\n","def augment_tranfer(\n","    config, device, TRACK_CODES, num_epochs, n_examples, checkpoint_version=None\n","  ):\n","  ''' assumes that the new version has all of the previous parameters '''\n","  af = instantiateAF(config, device)\n","  af.eval()\n","  af.is_training = True\n","  af.compute_loss = True\n","\n","  af.set_optim_config(opt_cfg, num_epochs, n_examples)\n","  af.track_codes(TRACK_CODES)\n","  af = af.to(device)\n","\n","  if checkpoint_version is not None:\n","    print('loading from checkpoint: '+str(checkpoint_version))\n","    path = 'lightning_logs/version_%s/checkpoints'%checkpoint_version\n","    [path] = list(glob.glob(path+'/*.ckpt'))\n","    state = torch.load(path)[\"state_dict\"]\n","\n","    # go through all parameters\n","    success, fail = [], []\n","    for npr, p in state.items():\n","      if npr in af.state_dict():\n","        af.state_dict()[npr] = p\n","        success.append(npr)\n","      else:\n","        fail.append(npr)\n","    print('SUCCESSFULLY TRANSFERRED:')\n","    print(success)\n","    print('FAILED TO TRANSFER:')\n","    print(fail)\n","  return af"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klJ-U4BadCad"},"source":["opt_cfg = {\n","  'optim_type' : 'AdamW',\n","  'optim_groups': {\n","    'default':{\n","      'lr':9e-5,\n","      'weight_decay':0,\n","    }\n","  },\n","  'scheduler': {\n","    'num_call_per_epoch':300,\n","    'class':'torch.optim.lr_scheduler.LambdaLR',\n","    'kwargs':\"{'lr_lambda':[lambda t: 0.55 + 0.45*np.cos(40*t)]}\"\n","  }\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KFwXy2zeJTC"},"source":["device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n","\n","config = model_config('model_5')\n","config.model.embeddings_and_evoformer.evoformer_num_block = 8#12\n","config.model.embeddings_and_evoformer.extra_msa_stack_num_block = 2\n","# config.model.heads.structure_module.num_layer = FOLD_LAYERS\n","config.data.training.constant.crop_size = 160\n","config.model.global_config.device = device\n","\n","num_epochs = 20\n","\n","data_args = (config, prepare_features, transform, device, 1e-6)\n","train_gen = RNAData(all_data['train'], *data_args, length=10000)\n","valid_gen = RNAData(all_data['validation'], *data_args, deterministic=True)\n","\n","af = standard_init(\n","  config, device, TRACK_CODES, num_epochs, len(train_gen), checkpoint_version=2\n",")\n","\n","# af = augment_tranfer(\n","#   config, device, TRACK_CODES, num_epochs, len(train_gen), checkpoint_version=2\n","# )\n","# for k in af.groups:\n","#   print('-'*100)\n","#   print(k)\n","#   print(af.groups[k])\n","#   print()\n","\n","# ei = 'evoformer_iteration'\n","# for n,p in af.named_parameters():\n","#   train_evo = False\n","#   if ei in n:\n","#     i = int(n[n.index(ei)+len(ei)+1:].split('.')[0])\n","#     train_evo = i >= 8\n","#   p.requires_grad = train_evo# or 'distogram' in n \n","\n","# for n,p in af.named_parameters():\n","#   print(n+', '+str(p.requires_grad))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJR38gMxl6C0"},"source":["data_conf = {'collate_fn':collate_fn, 'shuffle':True, 'batch_size':1,}\n","train_loader = DataLoader(train_gen, **data_conf)\n","data_conf['shuffle'] = False\n","val_loader = DataLoader(valid_gen, **data_conf)\n","\n","train_conf = {\n","    'auto_lr_find':False,\n","    'progress_bar_refresh_rate':1, \n","    'max_epochs':num_epochs, \n","    'gradient_clip_val':0.1, \n","    # 'plugins':DeepSpeedPlugin(\n","    #     stage=3,\n","    #     cpu_offload=True,  # Enable CPU Offloading\n","    #     partition_activations=True,\n","    #     # cpu_checkpointing=True,  # (Optional) offload activations to CPU\n","    # ),\n","}\n","if torch.cuda.is_available(): train_conf['gpus'] = 1\n","\n","trainer = Trainer(**train_conf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1HezVlaeqWb"},"source":["trainer.fit(af, train_loader, val_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZCUYxNmNWpG"},"source":[""],"execution_count":null,"outputs":[]}]}